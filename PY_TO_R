library(caret)
library(xgboost)
library(dplyr)

# 1. Split X and y
X <- model.matrix(~ . -1, data = df[, -which(names(df) == "y")])  # predictors (no intercept)
y <- df$y  # binary outcome (0/1)

# 2. Create 10-fold CV repeated 5 times
set.seed(2025)
folds <- createMultiFolds(y = y, k = 10, times = 5)

# 3. Initialize accuracy tracker
accuracies <- c()

# 4. Loop through each fold
for (i in seq_along(folds)) {
  train_idx <- folds[[i]]
  test_idx <- setdiff(seq_len(nrow(X)), train_idx)

  dtrain <- xgb.DMatrix(data = X[train_idx, ], label = y[train_idx])
  dtest  <- xgb.DMatrix(data = X[test_idx, ], label = y[test_idx])

  # Train the XGBoost model
  model <- xgboost(
    data = dtrain,
    objective = "binary:logistic",
    nrounds = 50,  # You can tune this later
    verbose = 0
  )

  # Predict on test data
  pred_probs <- predict(model, dtest)
  pred_labels <- ifelse(pred_probs >= 0.5, 1, 0)

  # Compute accuracy
  acc <- mean(pred_labels == y[test_idx])
  accuracies <- c(accuracies, acc)
}

# 5. Summary of results
mean_acc <- mean(accuracies)
sd_acc <- sd(accuracies)

cat("Repeated 10-fold CV Results (5x):\n")
cat("Mean Accuracy:", round(mean_acc, 4), "\n")
cat("SD Accuracy:", round(sd_acc, 4), "\n")
